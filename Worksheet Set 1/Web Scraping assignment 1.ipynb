{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) a python program to display all the header tags from wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping(url):\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    header=[]\n",
    "    \n",
    "    for i in soup.find_all('span',class_=\"mw-headline\"):\n",
    "        header.append(i.text)\n",
    "    return header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome to Wikipedia',\n",
       " \"From today's featured article\",\n",
       " 'Did you know\\xa0...',\n",
       " 'In the news',\n",
       " 'On this day',\n",
       " \"Today's featured picture\",\n",
       " 'Other areas of Wikipedia',\n",
       " \"Wikipedia's sister projects\",\n",
       " 'Wikipedia languages']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraping('https://en.wikipedia.org/wiki/Main_Page')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Python program to display IMDB’s Top rated 50 movies’ data (i.e. name, rating, year of release) and make data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    name=[]\n",
    "    \n",
    "    for i in soup.find_all('h3', class_=\"lister-item-header\"):\n",
    "        name.append(i.text.replace('\\n',' ').split()[1:])\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Shawshank', 'Redemption', '(1994)'],\n",
       " ['The', 'Godfather', '(1972)'],\n",
       " ['The', 'Dark', 'Knight', '(2008)'],\n",
       " ['The',\n",
       "  'Lord',\n",
       "  'of',\n",
       "  'the',\n",
       "  'Rings:',\n",
       "  'The',\n",
       "  'Return',\n",
       "  'of',\n",
       "  'the',\n",
       "  'King',\n",
       "  '(2003)'],\n",
       " [\"Schindler's\", 'List', '(1993)'],\n",
       " ['The', 'Godfather:', 'Part', 'II', '(1974)'],\n",
       " ['12', 'Angry', 'Men', '(1957)'],\n",
       " ['The',\n",
       "  'Lord',\n",
       "  'of',\n",
       "  'the',\n",
       "  'Rings:',\n",
       "  'The',\n",
       "  'Fellowship',\n",
       "  'of',\n",
       "  'the',\n",
       "  'Ring',\n",
       "  '(2001)'],\n",
       " ['Pulp', 'Fiction', '(1994)'],\n",
       " ['Inception', '(2010)'],\n",
       " ['The', 'Lord', 'of', 'the', 'Rings:', 'The', 'Two', 'Towers', '(2002)'],\n",
       " ['Fight', 'Club', '(1999)'],\n",
       " ['Forrest', 'Gump', '(1994)'],\n",
       " ['Il', 'buono,', 'il', 'brutto,', 'il', 'cattivo', '(1966)'],\n",
       " ['Spider-Man:', 'No', 'Way', 'Home', '(2021)'],\n",
       " ['Interstellar', '(2014)'],\n",
       " ['The', 'Matrix', '(1999)'],\n",
       " ['Goodfellas', '(1990)'],\n",
       " ['The', 'Empire', 'Strikes', 'Back', '(1980)'],\n",
       " ['One', 'Flew', 'Over', 'the', \"Cuckoo's\", 'Nest', '(1975)'],\n",
       " ['Shichinin', 'no', 'samurai', '(1954)'],\n",
       " [\"It's\", 'a', 'Wonderful', 'Life', '(1946)'],\n",
       " ['Cidade', 'de', 'Deus', '(2002)'],\n",
       " ['The', 'Pianist', '(2002)'],\n",
       " ['Sen', 'to', 'Chihiro', 'no', 'kamikakushi', '(2001)'],\n",
       " ['Saving', 'Private', 'Ryan', '(1998)'],\n",
       " ['The', 'Green', 'Mile', '(1999)'],\n",
       " ['La', 'vita', 'è', 'bella', '(1997)'],\n",
       " ['Se7en', '(1995)'],\n",
       " ['Léon', '(1994)'],\n",
       " ['Terminator', '2:', 'Judgment', 'Day', '(1991)'],\n",
       " ['The', 'Silence', 'of', 'the', 'Lambs', '(1991)'],\n",
       " ['Back', 'to', 'the', 'Future', '(1985)'],\n",
       " ['Star', 'Wars', '(1977)'],\n",
       " ['Seppuku', '(1962)'],\n",
       " ['Gisaengchung', '(2019)'],\n",
       " ['Avengers:', 'Infinity', 'War', '(2018)'],\n",
       " ['Whiplash', '(2014)'],\n",
       " ['Django', 'Unchained', '(2012)'],\n",
       " ['The', 'Intouchables', '(2011)'],\n",
       " ['3', 'Idiots', '(2009)'],\n",
       " ['The', 'Prestige', '(2006)'],\n",
       " ['The', 'Departed', '(2006)'],\n",
       " ['Memento', '(2000)'],\n",
       " ['Gladiator', '(2000)'],\n",
       " ['American', 'History', 'X', '(1998)'],\n",
       " ['The', 'Usual', 'Suspects', '(1995)'],\n",
       " ['The', 'Lion', 'King', '(1994)'],\n",
       " ['Nuovo', 'Cinema', 'Paradiso', '(1988)'],\n",
       " ['Hotaru', 'no', 'haka', '(1988)']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def release(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    release=[]\n",
    "    \n",
    "    for i in soup.find_all('span',class_=\"lister-item-year text-muted unbold\"):\n",
    "        release.append(i.text.replace('\\n',' '))\n",
    "    return release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(1994)',\n",
       " '(1972)',\n",
       " '(2008)',\n",
       " '(2003)',\n",
       " '(1993)',\n",
       " '(1974)',\n",
       " '(1957)',\n",
       " '(2001)',\n",
       " '(1994)',\n",
       " '(2010)',\n",
       " '(2002)',\n",
       " '(1999)',\n",
       " '(1994)',\n",
       " '(1966)',\n",
       " '(2021)',\n",
       " '(2014)',\n",
       " '(1999)',\n",
       " '(1990)',\n",
       " '(1980)',\n",
       " '(1975)',\n",
       " '(1954)',\n",
       " '(1946)',\n",
       " '(2002)',\n",
       " '(2002)',\n",
       " '(2001)',\n",
       " '(1998)',\n",
       " '(1999)',\n",
       " '(1997)',\n",
       " '(1995)',\n",
       " '(1994)',\n",
       " '(1991)',\n",
       " '(1991)',\n",
       " '(1985)',\n",
       " '(1977)',\n",
       " '(1962)',\n",
       " '(2019)',\n",
       " '(2018)',\n",
       " '(2014)',\n",
       " '(2012)',\n",
       " '(2011)',\n",
       " '(2009)',\n",
       " '(2006)',\n",
       " '(2006)',\n",
       " '(2000)',\n",
       " '(2000)',\n",
       " '(1998)',\n",
       " '(1995)',\n",
       " '(1994)',\n",
       " '(1988)',\n",
       " '(1988)']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "release(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    rating=[]\n",
    "    \n",
    "    for i in soup.find_all('div',class_=\"inline-block ratings-imdb-rating\"):\n",
    "        rating.append(i.text.replace('\\n',' '))\n",
    "    return rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  9.3 ',\n",
       " '  9.2 ',\n",
       " '  9.1 ',\n",
       " '  9.0 ',\n",
       " '  9.0 ',\n",
       " '  9.0 ',\n",
       " '  9.0 ',\n",
       " '  8.9 ',\n",
       " '  8.9 ',\n",
       " '  8.8 ',\n",
       " '  8.8 ',\n",
       " '  8.8 ',\n",
       " '  8.8 ',\n",
       " '  8.8 ',\n",
       " '  8.7 ',\n",
       " '  8.7 ',\n",
       " '  8.7 ',\n",
       " '  8.7 ',\n",
       " '  8.7 ',\n",
       " '  8.7 ',\n",
       " '  8.7 ',\n",
       " '  8.7 ',\n",
       " '  8.6 ',\n",
       " '  8.6 ',\n",
       " '  8.6 ',\n",
       " '  8.6 ',\n",
       " '  8.6 ',\n",
       " '  8.6 ',\n",
       " '  8.6 ',\n",
       " '  8.6 ',\n",
       " '  8.6 ',\n",
       " '  8.6 ',\n",
       " '  8.6 ',\n",
       " '  8.6 ',\n",
       " '  8.6 ',\n",
       " '  8.5 ',\n",
       " '  8.5 ',\n",
       " '  8.5 ',\n",
       " '  8.5 ',\n",
       " '  8.5 ',\n",
       " '  8.5 ',\n",
       " '  8.5 ',\n",
       " '  8.5 ',\n",
       " '  8.5 ',\n",
       " '  8.5 ',\n",
       " '  8.5 ',\n",
       " '  8.5 ',\n",
       " '  8.5 ',\n",
       " '  8.5 ',\n",
       " '  8.5 ']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie Name</th>\n",
       "      <th>Release year</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[The, Shawshank, Redemption, (1994)]</td>\n",
       "      <td>(1994)</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[The, Godfather, (1972)]</td>\n",
       "      <td>(1972)</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[The, Dark, Knight, (2008)]</td>\n",
       "      <td>(2008)</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The, Lord, of, the, Rings:, The, Return, of, ...</td>\n",
       "      <td>(2003)</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Schindler's, List, (1993)]</td>\n",
       "      <td>(1993)</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[The, Godfather:, Part, II, (1974)]</td>\n",
       "      <td>(1974)</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[12, Angry, Men, (1957)]</td>\n",
       "      <td>(1957)</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[The, Lord, of, the, Rings:, The, Fellowship, ...</td>\n",
       "      <td>(2001)</td>\n",
       "      <td>8.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[Pulp, Fiction, (1994)]</td>\n",
       "      <td>(1994)</td>\n",
       "      <td>8.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[Inception, (2010)]</td>\n",
       "      <td>(2010)</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[The, Lord, of, the, Rings:, The, Two, Towers,...</td>\n",
       "      <td>(2002)</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[Fight, Club, (1999)]</td>\n",
       "      <td>(1999)</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[Forrest, Gump, (1994)]</td>\n",
       "      <td>(1994)</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[Il, buono,, il, brutto,, il, cattivo, (1966)]</td>\n",
       "      <td>(1966)</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[Spider-Man:, No, Way, Home, (2021)]</td>\n",
       "      <td>(2021)</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[Interstellar, (2014)]</td>\n",
       "      <td>(2014)</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[The, Matrix, (1999)]</td>\n",
       "      <td>(1999)</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[Goodfellas, (1990)]</td>\n",
       "      <td>(1990)</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[The, Empire, Strikes, Back, (1980)]</td>\n",
       "      <td>(1980)</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[One, Flew, Over, the, Cuckoo's, Nest, (1975)]</td>\n",
       "      <td>(1975)</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[Shichinin, no, samurai, (1954)]</td>\n",
       "      <td>(1954)</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[It's, a, Wonderful, Life, (1946)]</td>\n",
       "      <td>(1946)</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[Cidade, de, Deus, (2002)]</td>\n",
       "      <td>(2002)</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[The, Pianist, (2002)]</td>\n",
       "      <td>(2002)</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[Sen, to, Chihiro, no, kamikakushi, (2001)]</td>\n",
       "      <td>(2001)</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[Saving, Private, Ryan, (1998)]</td>\n",
       "      <td>(1998)</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[The, Green, Mile, (1999)]</td>\n",
       "      <td>(1999)</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[La, vita, è, bella, (1997)]</td>\n",
       "      <td>(1997)</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[Se7en, (1995)]</td>\n",
       "      <td>(1995)</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[Léon, (1994)]</td>\n",
       "      <td>(1994)</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[Terminator, 2:, Judgment, Day, (1991)]</td>\n",
       "      <td>(1991)</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[The, Silence, of, the, Lambs, (1991)]</td>\n",
       "      <td>(1991)</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[Back, to, the, Future, (1985)]</td>\n",
       "      <td>(1985)</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[Star, Wars, (1977)]</td>\n",
       "      <td>(1977)</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[Seppuku, (1962)]</td>\n",
       "      <td>(1962)</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[Gisaengchung, (2019)]</td>\n",
       "      <td>(2019)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[Avengers:, Infinity, War, (2018)]</td>\n",
       "      <td>(2018)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[Whiplash, (2014)]</td>\n",
       "      <td>(2014)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[Django, Unchained, (2012)]</td>\n",
       "      <td>(2012)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[The, Intouchables, (2011)]</td>\n",
       "      <td>(2011)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[3, Idiots, (2009)]</td>\n",
       "      <td>(2009)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[The, Prestige, (2006)]</td>\n",
       "      <td>(2006)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[The, Departed, (2006)]</td>\n",
       "      <td>(2006)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[Memento, (2000)]</td>\n",
       "      <td>(2000)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[Gladiator, (2000)]</td>\n",
       "      <td>(2000)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[American, History, X, (1998)]</td>\n",
       "      <td>(1998)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[The, Usual, Suspects, (1995)]</td>\n",
       "      <td>(1995)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[The, Lion, King, (1994)]</td>\n",
       "      <td>(1994)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[Nuovo, Cinema, Paradiso, (1988)]</td>\n",
       "      <td>(1988)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[Hotaru, no, haka, (1988)]</td>\n",
       "      <td>(1988)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Movie Name Release year  Rating\n",
       "0                [The, Shawshank, Redemption, (1994)]       (1994)    9.3 \n",
       "1                            [The, Godfather, (1972)]       (1972)    9.2 \n",
       "2                         [The, Dark, Knight, (2008)]       (2008)    9.1 \n",
       "3   [The, Lord, of, the, Rings:, The, Return, of, ...       (2003)    9.0 \n",
       "4                         [Schindler's, List, (1993)]       (1993)    9.0 \n",
       "5                 [The, Godfather:, Part, II, (1974)]       (1974)    9.0 \n",
       "6                            [12, Angry, Men, (1957)]       (1957)    9.0 \n",
       "7   [The, Lord, of, the, Rings:, The, Fellowship, ...       (2001)    8.9 \n",
       "8                             [Pulp, Fiction, (1994)]       (1994)    8.9 \n",
       "9                                 [Inception, (2010)]       (2010)    8.8 \n",
       "10  [The, Lord, of, the, Rings:, The, Two, Towers,...       (2002)    8.8 \n",
       "11                              [Fight, Club, (1999)]       (1999)    8.8 \n",
       "12                            [Forrest, Gump, (1994)]       (1994)    8.8 \n",
       "13     [Il, buono,, il, brutto,, il, cattivo, (1966)]       (1966)    8.8 \n",
       "14               [Spider-Man:, No, Way, Home, (2021)]       (2021)    8.7 \n",
       "15                             [Interstellar, (2014)]       (2014)    8.7 \n",
       "16                              [The, Matrix, (1999)]       (1999)    8.7 \n",
       "17                               [Goodfellas, (1990)]       (1990)    8.7 \n",
       "18               [The, Empire, Strikes, Back, (1980)]       (1980)    8.7 \n",
       "19     [One, Flew, Over, the, Cuckoo's, Nest, (1975)]       (1975)    8.7 \n",
       "20                   [Shichinin, no, samurai, (1954)]       (1954)    8.7 \n",
       "21                 [It's, a, Wonderful, Life, (1946)]       (1946)    8.7 \n",
       "22                         [Cidade, de, Deus, (2002)]       (2002)    8.6 \n",
       "23                             [The, Pianist, (2002)]       (2002)    8.6 \n",
       "24        [Sen, to, Chihiro, no, kamikakushi, (2001)]       (2001)    8.6 \n",
       "25                    [Saving, Private, Ryan, (1998)]       (1998)    8.6 \n",
       "26                         [The, Green, Mile, (1999)]       (1999)    8.6 \n",
       "27                       [La, vita, è, bella, (1997)]       (1997)    8.6 \n",
       "28                                    [Se7en, (1995)]       (1995)    8.6 \n",
       "29                                     [Léon, (1994)]       (1994)    8.6 \n",
       "30            [Terminator, 2:, Judgment, Day, (1991)]       (1991)    8.6 \n",
       "31             [The, Silence, of, the, Lambs, (1991)]       (1991)    8.6 \n",
       "32                    [Back, to, the, Future, (1985)]       (1985)    8.6 \n",
       "33                               [Star, Wars, (1977)]       (1977)    8.6 \n",
       "34                                  [Seppuku, (1962)]       (1962)    8.6 \n",
       "35                             [Gisaengchung, (2019)]       (2019)    8.5 \n",
       "36                 [Avengers:, Infinity, War, (2018)]       (2018)    8.5 \n",
       "37                                 [Whiplash, (2014)]       (2014)    8.5 \n",
       "38                        [Django, Unchained, (2012)]       (2012)    8.5 \n",
       "39                        [The, Intouchables, (2011)]       (2011)    8.5 \n",
       "40                                [3, Idiots, (2009)]       (2009)    8.5 \n",
       "41                            [The, Prestige, (2006)]       (2006)    8.5 \n",
       "42                            [The, Departed, (2006)]       (2006)    8.5 \n",
       "43                                  [Memento, (2000)]       (2000)    8.5 \n",
       "44                                [Gladiator, (2000)]       (2000)    8.5 \n",
       "45                     [American, History, X, (1998)]       (1998)    8.5 \n",
       "46                     [The, Usual, Suspects, (1995)]       (1995)    8.5 \n",
       "47                          [The, Lion, King, (1994)]       (1994)    8.5 \n",
       "48                  [Nuovo, Cinema, Paradiso, (1988)]       (1988)    8.5 \n",
       "49                         [Hotaru, no, haka, (1988)]       (1988)    8.5 "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Movie Name':name(url),'Release year':release(url),'Rating':rating(url)})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.imdb.com/list/ls056092300/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    name=[]\n",
    "    \n",
    "    for i in soup.find_all('h3', class_=\"lister-item-header\"):\n",
    "        name.append(i.text.replace('\\n',' ').split()[1:])\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Ship', 'of', 'Theseus', '(2012)'],\n",
       " ['Iruvar', '(1997)'],\n",
       " ['Kaagaz', 'Ke', 'Phool', '(1959)'],\n",
       " ['Lagaan:', 'Once', 'Upon', 'a', 'Time', 'in', 'India', '(2001)'],\n",
       " ['Pather', 'Panchali', '(1955)'],\n",
       " ['Charulata', '(1964)'],\n",
       " ['Rang', 'De', 'Basanti', '(2006)'],\n",
       " ['Dev.D', '(2009)'],\n",
       " ['3', 'Idiots', '(2009)'],\n",
       " ['Awaara', '(1951)'],\n",
       " ['Nayakan', '(1987)'],\n",
       " ['Aparajito', '(1956)'],\n",
       " ['Pushpaka', 'Vimana', '(1987)'],\n",
       " ['Pyaasa', '(1957)'],\n",
       " ['Ghatashraddha', '(1977)'],\n",
       " ['Sholay', '(1975)'],\n",
       " ['Aradhana', '(1969)'],\n",
       " ['Do', 'Ankhen', 'Barah', 'Haath', '(1957)'],\n",
       " ['Bombay', '(1995)'],\n",
       " ['Neecha', 'Nagar', '(1946)'],\n",
       " ['Do', 'Bigha', 'Zamin', '(1953)'],\n",
       " ['Garm', 'Hava', '(1974)'],\n",
       " ['Piravi', '(1989)'],\n",
       " ['Mughal-E-Azam', '(1960)'],\n",
       " ['Amma', 'Ariyan', '(1986)'],\n",
       " ['Madhumati', '(1958)'],\n",
       " ['Goopy', 'Gyne', 'Bagha', 'Byne', '(1969)'],\n",
       " ['Gangs', 'of', 'Wasseypur', '(2012)'],\n",
       " ['Guide', '(1965)'],\n",
       " ['Satya', '(1998)'],\n",
       " ['Roja', '(1992)'],\n",
       " ['Mr.', 'India', '(1987)'],\n",
       " ['The', 'Cloud-Capped', 'Star', '(1960)'],\n",
       " ['Harishchandrachi', 'Factory', '(2009)'],\n",
       " ['Masoom', '(1983)'],\n",
       " ['Agneepath', '(1990)'],\n",
       " ['Tabarana', 'Kathe', '(1986)'],\n",
       " ['Zakhm', '(1998)'],\n",
       " ['Dil', 'Chahta', 'Hai', '(2001)'],\n",
       " ['Bhaag', 'Milkha', 'Bhaag', '(2013)'],\n",
       " ['Chupke', 'Chupke', '(1975)'],\n",
       " ['Dilwale', 'Dulhania', 'Le', 'Jayenge', '(1995)'],\n",
       " ['Taare', 'Zameen', 'Par', '(2007)'],\n",
       " ['Ardh', 'Satya', '(1983)'],\n",
       " ['Bhumika', '(1977)'],\n",
       " ['Enthiran', '(2010)'],\n",
       " ['Sadma', '(1983)'],\n",
       " ['Shwaas', '(2004)'],\n",
       " ['Lamhe', '(1991)'],\n",
       " ['Haqeeqat', '(1964)'],\n",
       " ['Shree', '420', '(1955)'],\n",
       " ['Kannathil', 'Muthamittal', '(2002)'],\n",
       " ['Hum', 'Aapke', 'Hain', 'Koun..!', '(1994)'],\n",
       " ['Ustad', 'Hotel', '(2012)'],\n",
       " ['Bandit', 'Queen', '(1994)'],\n",
       " ['Lakshya', '(2004)'],\n",
       " ['Black', 'Friday', '(2004)'],\n",
       " ['Manthan', '(1976)'],\n",
       " ['Apoorva', 'Raagangal', '(1975)'],\n",
       " ['English', 'Vinglish', '(2012)'],\n",
       " ['Jewel', 'Thief', '(1967)'],\n",
       " ['Pakeezah', '(1972)'],\n",
       " ['Maqbool', '(2003)'],\n",
       " ['Jis', 'Desh', 'Men', 'Ganga', 'Behti', 'Hai', '(1960)'],\n",
       " ['Sahib', 'Bibi', 'Aur', 'Ghulam', '(1962)'],\n",
       " ['Shatranj', 'Ke', 'Khilari', '(1977)'],\n",
       " ['Narthanasala', '(1963)'],\n",
       " ['Chandni', 'Bar', '(2001)'],\n",
       " ['Vaaranam', 'Aayiram', '(2008)'],\n",
       " ['Mr.', 'and', 'Mrs.', 'Iyer', '(2002)'],\n",
       " ['Chandni', '(1989)'],\n",
       " ['English,', 'August', '(1994)'],\n",
       " ['Celluloid', '(2013)'],\n",
       " ['Sagara', 'Sangamam', '(1983)'],\n",
       " ['Munna', 'Bhai', 'M.B.B.S.', '(2003)'],\n",
       " ['Saaransh', '(1984)'],\n",
       " ['Guddi', '(1971)'],\n",
       " ['Vanaja', '(2006)'],\n",
       " ['Vazhakku', 'Enn', '18/9', '(2012)'],\n",
       " ['Gangaajal', '(2003)'],\n",
       " ['Angoor', '(1982)'],\n",
       " ['Guru', '(2007)'],\n",
       " ['Andaz', 'Apna', 'Apna', '(1994)'],\n",
       " ['Sangam', '(I)', '(1964)'],\n",
       " ['Oka', 'Oori', 'Katha', '(1978)'],\n",
       " ['Bhuvan', 'Shome', '(1969)'],\n",
       " ['Border', '(I)', '(1997)'],\n",
       " ['Parineeta', '(2005)'],\n",
       " ['Devdas', '(1955)'],\n",
       " ['Abohomaan', '(2009)'],\n",
       " ['Kuch', 'Kuch', 'Hota', 'Hai', '(1998)'],\n",
       " ['Pithamagan', '(2003)'],\n",
       " ['Veyyil', '(2006)'],\n",
       " ['Chemmeen', '(1965)'],\n",
       " ['Jaane', 'Bhi', 'Do', 'Yaaro', '(1983)'],\n",
       " ['Apur', 'Sansar', '(1959)'],\n",
       " ['Kanchivaram', '(2008)'],\n",
       " ['Monsoon', 'Wedding', '(2001)'],\n",
       " ['Black', '(2005)'],\n",
       " ['Deewaar', '(1975)']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    rating=[]\n",
    "    \n",
    "    for i in soup.find_all('div',class_=\"ipl-rating-star small\"):\n",
    "        rating.append(i.text.replace('\\n',' '))\n",
    "    return rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['        8 ',\n",
       " '        8.5 ',\n",
       " '        7.8 ',\n",
       " '        8.1 ',\n",
       " '        8.4 ',\n",
       " '        8.1 ',\n",
       " '        8.1 ',\n",
       " '        8 ',\n",
       " '        8.5 ',\n",
       " '        7.8 ',\n",
       " '        8.7 ',\n",
       " '        8.3 ',\n",
       " '        8.6 ',\n",
       " '        8.3 ',\n",
       " '        7.6 ',\n",
       " '        8.1 ',\n",
       " '        7.6 ',\n",
       " '        8.4 ',\n",
       " '        8.1 ',\n",
       " '        6.8 ',\n",
       " '        8.2 ',\n",
       " '        8.1 ',\n",
       " '        7.8 ',\n",
       " '        8.2 ',\n",
       " '        7.1 ',\n",
       " '        7.8 ',\n",
       " '        8.7 ',\n",
       " '        8.2 ',\n",
       " '        8.4 ',\n",
       " '        8.3 ',\n",
       " '        8.2 ',\n",
       " '        7.8 ',\n",
       " '        7.9 ',\n",
       " '        8.4 ',\n",
       " '        8.4 ',\n",
       " '        7.6 ',\n",
       " '        7.9 ',\n",
       " '        7.9 ',\n",
       " '        8.1 ',\n",
       " '        8.3 ',\n",
       " '        8.3 ',\n",
       " '        8.2 ',\n",
       " '        8.4 ',\n",
       " '        8.1 ',\n",
       " '        7.4 ',\n",
       " '        7.1 ',\n",
       " '        8.3 ',\n",
       " '        8.2 ',\n",
       " '        7.3 ',\n",
       " '        7.9 ',\n",
       " '        7.9 ',\n",
       " '        8.4 ',\n",
       " '        7.5 ',\n",
       " '        8.3 ',\n",
       " '        7.5 ',\n",
       " '        7.9 ',\n",
       " '        8.5 ',\n",
       " '        7.6 ',\n",
       " '        7.6 ',\n",
       " '        7.9 ',\n",
       " '        7.9 ',\n",
       " '        7.2 ',\n",
       " '        8.1 ',\n",
       " '        7.1 ',\n",
       " '        8.1 ',\n",
       " '        7.5 ',\n",
       " '        8 ',\n",
       " '        7.6 ',\n",
       " '        8.4 ',\n",
       " '        7.9 ',\n",
       " '        6.7 ',\n",
       " '        7.6 ',\n",
       " '        7.8 ',\n",
       " '        8.9 ',\n",
       " '        8.1 ',\n",
       " '        8.1 ',\n",
       " '        7.2 ',\n",
       " '        7.2 ',\n",
       " '        8.3 ',\n",
       " '        7.8 ',\n",
       " '        8.3 ',\n",
       " '        7.7 ',\n",
       " '        8.1 ',\n",
       " '        7.3 ',\n",
       " '        7.7 ',\n",
       " '        7.3 ',\n",
       " '        8 ',\n",
       " '        7.2 ',\n",
       " '        7.7 ',\n",
       " '        7.4 ',\n",
       " '        7.7 ',\n",
       " '        8.4 ',\n",
       " '        7.8 ',\n",
       " '        7.8 ',\n",
       " '        8.4 ',\n",
       " '        8.5 ',\n",
       " '        8.1 ',\n",
       " '        7.3 ',\n",
       " '        8.2 ',\n",
       " '        8 ']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def release(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    release=[]\n",
    "    \n",
    "    for i in soup.find_all('span',class_=\"lister-item-year text-muted unbold\"):\n",
    "        release.append(i.text.replace('\\n',' '))\n",
    "    return release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(2012)',\n",
       " '(1997)',\n",
       " '(1959)',\n",
       " '(2001)',\n",
       " '(1955)',\n",
       " '(1964)',\n",
       " '(2006)',\n",
       " '(2009)',\n",
       " '(2009)',\n",
       " '(1951)',\n",
       " '(1987)',\n",
       " '(1956)',\n",
       " '(1987)',\n",
       " '(1957)',\n",
       " '(1977)',\n",
       " '(1975)',\n",
       " '(1969)',\n",
       " '(1957)',\n",
       " '(1995)',\n",
       " '(1946)',\n",
       " '(1953)',\n",
       " '(1974)',\n",
       " '(1989)',\n",
       " '(1960)',\n",
       " '(1986)',\n",
       " '(1958)',\n",
       " '(1969)',\n",
       " '(2012)',\n",
       " '(1965)',\n",
       " '(1998)',\n",
       " '(1992)',\n",
       " '(1987)',\n",
       " '(1960)',\n",
       " '(2009)',\n",
       " '(1983)',\n",
       " '(1990)',\n",
       " '(1986)',\n",
       " '(1998)',\n",
       " '(2001)',\n",
       " '(2013)',\n",
       " '(1975)',\n",
       " '(1995)',\n",
       " '(2007)',\n",
       " '(1983)',\n",
       " '(1977)',\n",
       " '(2010)',\n",
       " '(1983)',\n",
       " '(2004)',\n",
       " '(1991)',\n",
       " '(1964)',\n",
       " '(1955)',\n",
       " '(2002)',\n",
       " '(1994)',\n",
       " '(2012)',\n",
       " '(1994)',\n",
       " '(2004)',\n",
       " '(2004)',\n",
       " '(1976)',\n",
       " '(1975)',\n",
       " '(2012)',\n",
       " '(1967)',\n",
       " '(1972)',\n",
       " '(2003)',\n",
       " '(1960)',\n",
       " '(1962)',\n",
       " '(1977)',\n",
       " '(1963)',\n",
       " '(2001)',\n",
       " '(2008)',\n",
       " '(2002)',\n",
       " '(1989)',\n",
       " '(1994)',\n",
       " '(2013)',\n",
       " '(1983)',\n",
       " '(2003)',\n",
       " '(1984)',\n",
       " '(1971)',\n",
       " '(2006)',\n",
       " '(2012)',\n",
       " '(2003)',\n",
       " '(1982)',\n",
       " '(2007)',\n",
       " '(1994)',\n",
       " '(I) (1964)',\n",
       " '(1978)',\n",
       " '(1969)',\n",
       " '(I) (1997)',\n",
       " '(2005)',\n",
       " '(1955)',\n",
       " '(2009)',\n",
       " '(1998)',\n",
       " '(2003)',\n",
       " '(2006)',\n",
       " '(1965)',\n",
       " '(1983)',\n",
       " '(1959)',\n",
       " '(2008)',\n",
       " '(2001)',\n",
       " '(2005)',\n",
       " '(1975)']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "release(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie Name</th>\n",
       "      <th>Release year</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Ship, of, Theseus, (2012)]</td>\n",
       "      <td>(2012)</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Iruvar, (1997)]</td>\n",
       "      <td>(1997)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Kaagaz, Ke, Phool, (1959)]</td>\n",
       "      <td>(1959)</td>\n",
       "      <td>7.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Lagaan:, Once, Upon, a, Time, in, India, (2001)]</td>\n",
       "      <td>(2001)</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Pather, Panchali, (1955)]</td>\n",
       "      <td>(1955)</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>[Apur, Sansar, (1959)]</td>\n",
       "      <td>(1959)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>[Kanchivaram, (2008)]</td>\n",
       "      <td>(2008)</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>[Monsoon, Wedding, (2001)]</td>\n",
       "      <td>(2001)</td>\n",
       "      <td>7.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>[Black, (2005)]</td>\n",
       "      <td>(2005)</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>[Deewaar, (1975)]</td>\n",
       "      <td>(1975)</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Movie Name Release year  \\\n",
       "0                         [Ship, of, Theseus, (2012)]       (2012)   \n",
       "1                                    [Iruvar, (1997)]       (1997)   \n",
       "2                         [Kaagaz, Ke, Phool, (1959)]       (1959)   \n",
       "3   [Lagaan:, Once, Upon, a, Time, in, India, (2001)]       (2001)   \n",
       "4                          [Pather, Panchali, (1955)]       (1955)   \n",
       "..                                                ...          ...   \n",
       "95                             [Apur, Sansar, (1959)]       (1959)   \n",
       "96                              [Kanchivaram, (2008)]       (2008)   \n",
       "97                         [Monsoon, Wedding, (2001)]       (2001)   \n",
       "98                                    [Black, (2005)]       (2005)   \n",
       "99                                  [Deewaar, (1975)]       (1975)   \n",
       "\n",
       "          Rating  \n",
       "0             8   \n",
       "1           8.5   \n",
       "2           7.8   \n",
       "3           8.1   \n",
       "4           8.4   \n",
       "..           ...  \n",
       "95          8.5   \n",
       "96          8.1   \n",
       "97          7.3   \n",
       "98          8.2   \n",
       "99            8   \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Movie Name':name(url),'Release year':release(url),'Rating':rating(url)})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Python Program to scrape product name, price and discounts from https://meesho.com/bags-ladies/pl/p7vbp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=('https://meesho.com/bags-ladies/pl/p7vbp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    name=[]\n",
    "    \n",
    "    for i in soup.find_all('p', class_=\"Text__StyledText-sc-oo0kvp-0 bWSOET NewProductCard__ProductTitle_Desktop-sc-j0e7tu-4 cQhePS NewProductCard__ProductTitle_Desktop-sc-j0e7tu-4 cQhePS\"):\n",
    "        name.append(i.text.replace('\\n',' '))\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Voguish Fashionable Women Handbags',\n",
       " 'Elegant Stylish Women Handbags',\n",
       " 'Gorgeous Classy Women Handbags',\n",
       " \"Elegant Fancy Women'S Pu Leather Hand Bags\",\n",
       " \"Elegant Fancy Women's Bags\",\n",
       " 'Ravishing Alluring Women Handbags',\n",
       " 'Voguish Fashionable Women Handbags',\n",
       " 'Trendy Fancy Women Handbags',\n",
       " 'Gorgeous Versatile Women Handbags',\n",
       " 'Gorgeous Attractive Women Handbags',\n",
       " 'Elite Fancy Women Handbags',\n",
       " 'Attractive Women Handbags',\n",
       " 'Wonderful Women Women Handbags',\n",
       " 'Elite Fancy Women Handbags',\n",
       " 'Ravishing Versatile Women Handbags',\n",
       " 'Trendy Attractive Women Handbags',\n",
       " 'Classic Attractive Women Handbag',\n",
       " 'Elegant Stylish Women Handbags',\n",
       " 'Graceful Attractive Women Handbags',\n",
       " 'Voguish Alluring Women Handbags']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    price=[]\n",
    "    \n",
    "    for i in soup.find_all('h5',class_=\"Text__StyledText-sc-oo0kvp-0 hiHdyy\"):\n",
    "        price.append(i.text)\n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['₹851',\n",
       " '₹186',\n",
       " '₹193',\n",
       " '₹175',\n",
       " '₹189',\n",
       " '₹130',\n",
       " '₹254',\n",
       " '₹210',\n",
       " '₹222',\n",
       " '₹196',\n",
       " '₹140',\n",
       " '₹197',\n",
       " '₹217',\n",
       " '₹260',\n",
       " '₹260',\n",
       " '₹207',\n",
       " '₹204',\n",
       " '₹182',\n",
       " '₹133',\n",
       " '₹517']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    discount=[]\n",
    "    \n",
    "    for i in soup.find_all('span',class_=\"Text__StyledText-sc-oo0kvp-0 lnonyH\"):\n",
    "        discount.append(i.text.split()[:1])\n",
    "    return discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['11%'],\n",
       " ['30%'],\n",
       " ['30%'],\n",
       " ['30%'],\n",
       " ['30%'],\n",
       " ['30%'],\n",
       " ['28%'],\n",
       " ['30%'],\n",
       " ['30%'],\n",
       " ['30%'],\n",
       " ['30%'],\n",
       " ['30%'],\n",
       " ['30%'],\n",
       " ['28%'],\n",
       " ['28%'],\n",
       " ['30%'],\n",
       " ['30%'],\n",
       " ['30%'],\n",
       " ['30%'],\n",
       " ['16%']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Voguish Fashionable Women Handbags</td>\n",
       "      <td>₹851</td>\n",
       "      <td>[11%]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Elegant Stylish Women Handbags</td>\n",
       "      <td>₹186</td>\n",
       "      <td>[30%]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gorgeous Classy Women Handbags</td>\n",
       "      <td>₹193</td>\n",
       "      <td>[30%]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elegant Fancy Women'S Pu Leather Hand Bags</td>\n",
       "      <td>₹175</td>\n",
       "      <td>[30%]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elegant Fancy Women's Bags</td>\n",
       "      <td>₹189</td>\n",
       "      <td>[30%]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ravishing Alluring Women Handbags</td>\n",
       "      <td>₹130</td>\n",
       "      <td>[30%]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Voguish Fashionable Women Handbags</td>\n",
       "      <td>₹254</td>\n",
       "      <td>[28%]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Trendy Fancy Women Handbags</td>\n",
       "      <td>₹210</td>\n",
       "      <td>[30%]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Gorgeous Versatile Women Handbags</td>\n",
       "      <td>₹222</td>\n",
       "      <td>[30%]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Gorgeous Attractive Women Handbags</td>\n",
       "      <td>₹196</td>\n",
       "      <td>[30%]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Elite Fancy Women Handbags</td>\n",
       "      <td>₹140</td>\n",
       "      <td>[30%]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Attractive Women Handbags</td>\n",
       "      <td>₹197</td>\n",
       "      <td>[30%]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Wonderful Women Women Handbags</td>\n",
       "      <td>₹217</td>\n",
       "      <td>[30%]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Elite Fancy Women Handbags</td>\n",
       "      <td>₹260</td>\n",
       "      <td>[28%]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Ravishing Versatile Women Handbags</td>\n",
       "      <td>₹260</td>\n",
       "      <td>[28%]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Trendy Attractive Women Handbags</td>\n",
       "      <td>₹207</td>\n",
       "      <td>[30%]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Classic Attractive Women Handbag</td>\n",
       "      <td>₹204</td>\n",
       "      <td>[30%]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Elegant Stylish Women Handbags</td>\n",
       "      <td>₹182</td>\n",
       "      <td>[30%]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Graceful Attractive Women Handbags</td>\n",
       "      <td>₹133</td>\n",
       "      <td>[30%]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Voguish Alluring Women Handbags</td>\n",
       "      <td>₹517</td>\n",
       "      <td>[16%]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Product Name Price Discount\n",
       "0           Voguish Fashionable Women Handbags  ₹851    [11%]\n",
       "1               Elegant Stylish Women Handbags  ₹186    [30%]\n",
       "2               Gorgeous Classy Women Handbags  ₹193    [30%]\n",
       "3   Elegant Fancy Women'S Pu Leather Hand Bags  ₹175    [30%]\n",
       "4                   Elegant Fancy Women's Bags  ₹189    [30%]\n",
       "5            Ravishing Alluring Women Handbags  ₹130    [30%]\n",
       "6           Voguish Fashionable Women Handbags  ₹254    [28%]\n",
       "7                  Trendy Fancy Women Handbags  ₹210    [30%]\n",
       "8            Gorgeous Versatile Women Handbags  ₹222    [30%]\n",
       "9           Gorgeous Attractive Women Handbags  ₹196    [30%]\n",
       "10                  Elite Fancy Women Handbags  ₹140    [30%]\n",
       "11                   Attractive Women Handbags  ₹197    [30%]\n",
       "12              Wonderful Women Women Handbags  ₹217    [30%]\n",
       "13                  Elite Fancy Women Handbags  ₹260    [28%]\n",
       "14          Ravishing Versatile Women Handbags  ₹260    [28%]\n",
       "15            Trendy Attractive Women Handbags  ₹207    [30%]\n",
       "16            Classic Attractive Women Handbag  ₹204    [30%]\n",
       "17              Elegant Stylish Women Handbags  ₹182    [30%]\n",
       "18          Graceful Attractive Women Handbags  ₹133    [30%]\n",
       "19             Voguish Alluring Women Handbags  ₹517    [16%]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Product Name':name(url),'Price':price(url),'Discount':discount(url)})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) a python program to scrape cricket rankings from https://www.icc-cricket.com/rankings/mens/player-rankings/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=('https://www.icc-cricket.com/rankings/mens/team-rankings/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  Australia AUS ']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a)\n",
    "def teams(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    teams=[]\n",
    "    \n",
    "    for i in soup.find_all('td',class_=\"rankings-block__banner--team-name\"):\n",
    "        teams.append(i.text.replace('\\n',' '))\n",
    "    return teams\n",
    "\n",
    "teams(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['  Australia AUS '] ['23'] ['2,736'] ['119']\n"
     ]
    }
   ],
   "source": [
    "def matches(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    matches=[]\n",
    "    \n",
    "    for i in soup.find_all('td',class_=\"rankings-block__banner--matches\"):\n",
    "        matches.append(i.text)\n",
    "    return matches\n",
    "\n",
    "matches(url)\n",
    "\n",
    "def points(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    points=[]\n",
    "    \n",
    "    for i in soup.find_all('td',class_=\"rankings-block__banner--points\"):\n",
    "        points.append(i.text)\n",
    "    return points\n",
    "points(url)\n",
    "\n",
    "def rating(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    rating=[]\n",
    "    \n",
    "    for i in soup.find_all('td',class_=\"rankings-block__banner--rating u-text-right\"):\n",
    "        rating.append(i.text.split()[0])\n",
    "    return rating\n",
    "rating(url)\n",
    "\n",
    "\n",
    "print(teams(url),matches(url),points(url),rating(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['  Australia AUS '] ['23'] ['2,736'] ['119']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' 2   New Zealand NZ  28 3,264 117 ',\n",
       " ' 3   India IND  32 3,717 116 ',\n",
       " ' 4   England ENG  41 4,151 101 ',\n",
       " ' 5   South Africa SA  23 2,271 99 ',\n",
       " ' 6   Pakistan PAK  30 2,787 93 ',\n",
       " ' 7   Sri Lanka SL  30 2,485 83 ',\n",
       " ' 8   West Indies WI  33 2,480 75 ',\n",
       " ' 9   Bangladesh BAN  22 1,157 53 ',\n",
       " ' 10   Zimbabwe ZIM  11 342 31 ']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( teams(url),matches(url),points(url),rating(url))\n",
    "def other_teams(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    other_teams=[]\n",
    "    \n",
    "    for i in soup.find_all('tr',class_=\"table-body\"):\n",
    "        other_teams.append(i.text.replace('\\n',' '))\n",
    "    return other_teams\n",
    "other_teams(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Babar Azam'] ['PAK'] ['873']\n"
     ]
    }
   ],
   "source": [
    "#b)\n",
    "url='https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "def batsmen(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    batsmen=[]\n",
    "    \n",
    "    for i in soup.find_all('div',class_=\"rankings-block__banner--name-large\"):\n",
    "        batsmen.append(i.text)\n",
    "    return batsmen\n",
    "batsmen(url)\n",
    "\n",
    "def teams(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    teams=[]\n",
    "    \n",
    "    for i in soup.find_all('div',class_=\"rankings-block__banner--nationality\"):\n",
    "        teams.append(i.text.replace('\\n',' ').split()[0])\n",
    "    return teams\n",
    "\n",
    "teams(url)\n",
    "\n",
    "def rating(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    rating=[]\n",
    "    \n",
    "    for i in soup.find_all('td',class_=\"u-text-left\"):\n",
    "        rating.append(i.text.split()[0])\n",
    "    return rating\n",
    "rating(url)\n",
    "\n",
    "print(batsmen(url),teams(url),rating(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Babar Azam'] ['PAK'] ['873']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Virat', 'Kohli', 'IND', '811'],\n",
       " ['Rohit', 'Sharma', 'IND', '791'],\n",
       " ['Quinton', 'de', 'Kock', 'SA'],\n",
       " ['Aaron', 'Finch', 'AUS', '779'],\n",
       " ['Jonny', 'Bairstow', 'ENG', '775'],\n",
       " ['David', 'Warner', 'AUS', '762'],\n",
       " ['Rassie', 'van', 'der', 'Dussen'],\n",
       " ['Fakhar', 'Zaman', 'PAK', '741'],\n",
       " ['Joe', 'Root', 'ENG', '740'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['Steve', 'Smith', 'AUS', '696'],\n",
       " ['Jason', 'Roy', 'ENG', '687'],\n",
       " ['Glenn', 'Maxwell', 'AUS', '674'],\n",
       " ['Tamim', 'Iqbal', 'BAN', '670'],\n",
       " ['Alex', 'Carey', 'AUS', '669'],\n",
       " ['Paul', 'Stirling', 'IRE', '660'],\n",
       " ['Ben', 'Stokes', 'ENG', '651'],\n",
       " ['Eoin', 'Morgan', 'ENG', '650'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['Tom', 'Latham', 'NZ', '624'],\n",
       " ['David', 'Miller', 'SA', '618'],\n",
       " ['Jos', 'Buttler', 'ENG', '608'],\n",
       " ['Janneman', 'Malan', 'SA', '594'],\n",
       " ['Haris', 'Sohail', 'PAK', '594'],\n",
       " ['Shimron', 'Hetmyer', 'WI', '589'],\n",
       " ['Henry', 'Nicholls', 'NZ', '581'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['Mahmudullah', 'BAN', '571', '582'],\n",
       " ['Aqib', 'Ilyas', 'OMA', '565'],\n",
       " ['Avishka', 'Fernando', 'SL', '560'],\n",
       " ['Sean', 'Williams', 'ZIM', '559'],\n",
       " ['Litton', 'Das', 'BAN', '558'],\n",
       " ['Kariyawasa', 'Asalanka', 'SL', '546'],\n",
       " ['Harry', 'Tector', 'IRE', '544'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['Sikandar', 'Raza', 'ZIM', '540'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['Hashmatullah', 'Shaidi', 'AFG', '528'],\n",
       " ['Imad', 'Wasim', 'PAK', '525'],\n",
       " ['Danushka', 'Gunathilaka', 'SL', '522'],\n",
       " ['Soumya', 'Sarkar', 'BAN', '521'],\n",
       " ['Marcus', 'Stoinis', 'AUS', '517'],\n",
       " ['Najibullah', 'Zadran', 'AFG', '517'],\n",
       " ['Temba', 'Bavuma', 'SA', '512'],\n",
       " ['Kusal', 'Mendis', 'SL', '511'],\n",
       " ['Sarfaraz', 'Ahmed', 'PAK', '504'],\n",
       " ['Calum', 'MacLeod', 'SCO', '500'],\n",
       " ['Marnus', 'Labuschagne', 'AUS', '498'],\n",
       " ['Jimmy', 'Neesham', 'NZ', '495'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['Mohammad', 'Rizwan', 'PAK', '414'],\n",
       " ['Dinesh', 'Chandimal', 'SL', '414'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['Gulbadin', 'Naib', 'AFG', '406'],\n",
       " ['Matthew', 'Cross', 'SCO', '405'],\n",
       " ['Moeen', 'Ali', 'ENG', '404'],\n",
       " ['Curtis', 'Campher', 'IRE', '403'],\n",
       " ['Chris', 'Woakes', 'ENG', '403'],\n",
       " ['Scott', 'Edwards', 'NED', '402'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['Craig', 'Williams', 'NAM', '399'],\n",
       " ['Sam', 'Billings', 'ENG', '396'],\n",
       " ['Monank', 'Patel', 'USA', '394'],\n",
       " ['Jatinder', 'Singh', 'OMA', '394'],\n",
       " ['Rahmanullah', 'Gurbaz', 'AFG', '393']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(batsmen(url),teams(url),rating(url))\n",
    "def other_players(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    other_players=[]\n",
    "    \n",
    "    for i in soup.find_all('tr',class_=\"table-body\"):\n",
    "        other_players.append(i.text.replace('\\n',' ').split()[2:6])\n",
    "    return other_players\n",
    "other_players(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Trent Boult'] ['NZ'] ['737']\n"
     ]
    }
   ],
   "source": [
    "#c)\n",
    "url='https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'\n",
    "\n",
    "def bowlers(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    bowlers=[]\n",
    "    \n",
    "    for i in soup.find_all('div',class_=\"rankings-block__banner--name-large\"):\n",
    "        bowlers.append(i.text)\n",
    "    return bowlers\n",
    "bowlers(url)\n",
    "\n",
    "def teams(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    teams=[]\n",
    "    \n",
    "    for i in soup.find_all('div',class_=\"rankings-block__banner--nationality\"):\n",
    "        teams.append(i.text.replace('\\n',' ').split()[0])\n",
    "    return teams\n",
    "\n",
    "teams(url)\n",
    "\n",
    "def rating(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    rating=[]\n",
    "    \n",
    "    for i in soup.find_all('td',class_=\"u-text-left\"):\n",
    "        rating.append(i.text.split()[0])\n",
    "    return rating\n",
    "rating(url)\n",
    "\n",
    "print(batsmen(url),teams(url),rating(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Trent Boult'] ['NZ'] ['737']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Josh', 'Hazlewood', 'AUS', '709'],\n",
       " ['Chris', 'Woakes', 'ENG', '700'],\n",
       " ['Mujeeb', 'Ur', 'Rahman', 'AFG'],\n",
       " ['Mehedi', 'Hasan', 'BAN', '692'],\n",
       " ['Matt', 'Henry', 'NZ', '691'],\n",
       " ['Jasprit', 'Bumrah', 'IND', '679'],\n",
       " ['Mitchell', 'Starc', 'AUS', '652'],\n",
       " ['Shakib', 'Al', 'Hasan', 'BAN'],\n",
       " ['Andy', 'McBrine', 'IRE', '646'],\n",
       " ['Mustafizur', 'Rahman', 'BAN', '640'],\n",
       " ['Pat', 'Cummins', 'AUS', '636'],\n",
       " ['Kagiso', 'Rabada', 'SA', '633'],\n",
       " ['Shaheen', 'Afridi', 'PAK', '630'],\n",
       " ['Rashid', 'Khan', 'AFG', '626'],\n",
       " ['Adam', 'Zampa', 'AUS', '625'],\n",
       " ['Yuzvendra', 'Chahal', 'IND', '610'],\n",
       " ['Lungi', 'Ngidi', 'SA', '608'],\n",
       " ['Mark', 'Wood', 'ENG', '594'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['Mohammad', 'Nabi', 'AFG', '587'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['Mitchell', 'Santner', 'NZ', '583'],\n",
       " ['Akila', 'Dananjaya', 'SL', '574'],\n",
       " ['Tabraiz', 'Shamsi', 'SA', '564'],\n",
       " ['Mohammad', 'Shami', 'IND', '549'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['Ravindra', 'Jadeja', 'IND', '534'],\n",
       " ['Keshav', 'Maharaj', 'SA', '534'],\n",
       " ['Sheldon', 'Cottrell', 'WI', '525'],\n",
       " ['Wanindu', 'Hasaranga', 'SL', '523'],\n",
       " ['Mark', 'Watt', 'SCO', '519'],\n",
       " ['Shadab', 'Khan', 'PAK', '517'],\n",
       " ['David', 'Willey', 'ENG', '501'],\n",
       " ['Simi', 'Singh', 'IRE', '499'],\n",
       " ['Dwaine', 'Pretorius', 'SA', '497'],\n",
       " ['Adil', 'Rashid', 'ENG', '497'],\n",
       " ['Imad', 'Wasim', 'PAK', '488'],\n",
       " ['Rohan', 'Mustafa', 'UAE', '484'],\n",
       " ['Blessing', 'Muzarabani', 'ZIM', '484'],\n",
       " ['Hasan', 'Ali', 'PAK', '483'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['Tom', 'Curran', 'ENG', '436'],\n",
       " ['Safyaan', 'Sharif', 'SCO', '436'],\n",
       " ['Ashton', 'Agar', 'AUS', '436'],\n",
       " ['Taskin', 'Ahmed', 'BAN', '434'],\n",
       " ['Suranga', 'Lakmal', 'SL', '429'],\n",
       " ['Anrich', 'Nortje', 'SA', '423'],\n",
       " ['Saurabh', 'Netravalkar', 'USA', '419'],\n",
       " ['Ben', 'Stokes', 'ENG', '410'],\n",
       " ['Alasdair', 'Evans', 'SCO', '408'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['Sean', 'Williams', 'ZIM', '401'],\n",
       " ['Sandeep', 'Lamichhane', 'NEP', '400'],\n",
       " ['Rubel', 'Hossain', 'BAN', '400'],\n",
       " ['Chad', 'Soper', 'PNG', '397'],\n",
       " ['Fred', 'Klaassen', 'NED', '395'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['Mitchell', 'Marsh', 'AUS', '390'],\n",
       " ['Hardik', 'Pandya', 'IND', '382'],\n",
       " ['Ravichandran', 'Ashwin', 'IND', '381'],\n",
       " ['Wayne', 'Parnell', 'SA', '381'],\n",
       " ['Maheesh', 'Theekshana', 'SL', '380'],\n",
       " ['Nosaina', 'Pokana', 'PNG', '380'],\n",
       " ['Barry', 'McCarthy', 'IRE', '379'],\n",
       " ['Marcus', 'Stoinis', 'AUS', '377'],\n",
       " ['Michael', 'Leask', 'SCO', '374'],\n",
       " ['Mohammad', 'Nawaz', 'PAK', '370'],\n",
       " ['Assad', 'Vala', 'PNG', '370'],\n",
       " ['Jeffrey', 'Vandersay', 'SL', '365'],\n",
       " ['Lakshan', 'Sandakan', 'SL', '365'],\n",
       " ['Richard', 'Ngarava', 'ZIM', '364'],\n",
       " ['Kaleemullah', 'OMA', '364', '364'],\n",
       " ['Roston', 'Chase', 'WI', '362'],\n",
       " ['Zahoor', 'Khan', 'UAE', '361'],\n",
       " ['Sharafuddin', 'Ashraf', 'AFG', '361'],\n",
       " ['Mark', 'Adair', 'IRE', '357'],\n",
       " ['Donald', 'Tiripano', 'ZIM', '356'],\n",
       " ['George', 'Dockrell', 'IRE', '353'],\n",
       " ['Kyle', 'Jamieson', 'NZ', '352']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(batsmen(url),teams(url),rating(url))\n",
    "def other_players(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    other_players=[]\n",
    "    \n",
    "    for i in soup.find_all('tr',class_=\"table-body\"):\n",
    "        other_players.append(i.text.replace('\\n',' ').split()[2:6])\n",
    "    return other_players\n",
    "other_players(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) a python program to scrape cricket rankings from https://www.icc-cricket.com/rankings/womens/player-rankings/odi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.icc-cricket.com/rankings/womens/team-rankings/odi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  Australia AUS ']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a)\n",
    "def teams(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    teams=[]\n",
    "    \n",
    "    for i in soup.find_all('td',class_=\"rankings-block__banner--team-name\"):\n",
    "        teams.append(i.text.replace('\\n',' '))\n",
    "    return teams\n",
    "\n",
    "teams(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['  Australia AUS '] ['20'] ['3,263'] ['163']\n"
     ]
    }
   ],
   "source": [
    "def matches(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    matches=[]\n",
    "    \n",
    "    for i in soup.find_all('td',class_=\"rankings-block__banner--matches\"):\n",
    "        matches.append(i.text)\n",
    "    return matches\n",
    "\n",
    "matches(url)\n",
    "\n",
    "def points(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    points=[]\n",
    "    \n",
    "    for i in soup.find_all('td',class_=\"rankings-block__banner--points\"):\n",
    "        points.append(i.text)\n",
    "    return points\n",
    "points(url)\n",
    "\n",
    "def rating(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    rating=[]\n",
    "    \n",
    "    for i in soup.find_all('td',class_=\"rankings-block__banner--rating u-text-right\"):\n",
    "        rating.append(i.text.split()[0])\n",
    "    return rating\n",
    "rating(url)\n",
    "\n",
    "\n",
    "print(teams(url),matches(url),points(url),rating(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['  Australia AUS '] ['20'] ['3,263'] ['163']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' 2   South Africa SA  21 2,580 123 ',\n",
       " ' 3   England ENG  21 2,474 118 ',\n",
       " ' 4   India IND  22 2,221 101 ',\n",
       " ' 5   New Zealand NZ  24 2,342 98 ',\n",
       " ' 6   Bangladesh BAN  5 475 95 ',\n",
       " ' 7   West Indies WI  21 1,801 86 ',\n",
       " ' 8   Pakistan PAK  19 1,304 69 ',\n",
       " ' 9   Ireland IRE  5 240 48 ',\n",
       " ' 10   Sri Lanka SL  5 233 47 ',\n",
       " ' 11   Zimbabwe ZIM  8 0 0 ']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( teams(url),matches(url),points(url),rating(url))\n",
    "def other_teams(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    other_teams=[]\n",
    "    \n",
    "    for i in soup.find_all('tr',class_=\"table-body\"):\n",
    "        other_teams.append(i.text.replace('\\n',' '))\n",
    "    return other_teams\n",
    "other_teams(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alyssa Healy'] ['AUS'] ['749']\n"
     ]
    }
   ],
   "source": [
    "#b\n",
    "url='https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting'\n",
    "def batsmen(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    batsmen=[]\n",
    "    \n",
    "    for i in soup.find_all('div',class_=\"rankings-block__banner--name-large\"):\n",
    "        batsmen.append(i.text)\n",
    "    return batsmen\n",
    "batsmen(url)\n",
    "\n",
    "def teams(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    teams=[]\n",
    "    \n",
    "    for i in soup.find_all('div',class_=\"rankings-block__banner--nationality\"):\n",
    "        teams.append(i.text.replace('\\n',' ').split()[0])\n",
    "    return teams\n",
    "\n",
    "teams(url)\n",
    "\n",
    "def rating(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    rating=[]\n",
    "    \n",
    "    for i in soup.find_all('td',class_=\"u-text-left\"):\n",
    "        rating.append(i.text.split()[0])\n",
    "    return rating\n",
    "rating(url)\n",
    "\n",
    "print(batsmen(url),teams(url),rating(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alyssa Healy'] ['AUS'] ['749']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Mithali', 'Raj', 'IND', '735'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['Rachael', 'Haynes', 'AUS', '640'],\n",
       " ['Heather', 'Knight', 'ENG', '629'],\n",
       " ['Mignon', 'du', 'Preez', 'SA'],\n",
       " ['Deandra', 'Dottin', 'WI', '616'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['Ashleigh', 'Gardner', 'AUS', '509'],\n",
       " ['Aliya', 'Riaz', 'PAK', '504'],\n",
       " ['Marizanne', 'Kapp', 'SA', '496'],\n",
       " ['Poonam', 'Raut', 'IND', '485'],\n",
       " ['Rumana', 'Ahmed', 'BAN', '476'],\n",
       " ['Nahida', 'Khan', 'PAK', '458'],\n",
       " ['Javeria', 'Khan', 'PAK', '457'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['Gaby', 'Lewis', 'IRE', '438'],\n",
       " ['Danielle', 'Wyatt', 'ENG', '438'],\n",
       " ['Nida', 'Dar', 'PAK', '425'],\n",
       " ['Katherine', 'Brunt', 'ENG', '422'],\n",
       " ['Chloe-Lesleigh', 'Tryon', 'SA', '416'],\n",
       " ['Lauren', 'Winfield', 'ENG', '414'],\n",
       " ['Sune', 'Luus', 'SA', '414'],\n",
       " ['Tahlia', 'McGrath', 'AUS', '405'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['Katey', 'Martin', 'NZ', '380'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['Umaima', 'Sohail', 'PAK', '369'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['Jess', 'Jonassen', 'AUS', '346'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['Muneeba', 'Ali', 'PAK', '320'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['Murshida', 'Khatun', 'BAN', '300'],\n",
       " ['Tazmin', 'Brits', 'SA', '291'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['Shemaine', 'Campbelle', 'WI', '276'],\n",
       " ['Lara', 'Goodall', 'SA', '266'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['Sabbhineni', 'Meghana', 'IND', '250'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['Mary', 'Waldron', 'IRE', '248'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['Sidra', 'Ameen', 'PAK', '233'],\n",
       " ['Ritu', 'Moni', 'BAN', '231'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['Georgia', 'Wareham', 'AUS', '223'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['Eimear', 'Richardson', 'IRE', '217'],\n",
       " ['Poonam', 'Yadav', 'IND', '211'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['Sinalo', 'Jafta', 'SA', '206'],\n",
       " ['Sophie', 'Ecclestone', 'ENG', '204'],\n",
       " ['Rashada', 'Williams', 'WI', '203'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['This', 'player', 'has', 'moved'],\n",
       " ['Orla', 'Prendergast', 'IRE', '193']]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(batsmen(url),teams(url),rating(url))\n",
    "def other_players(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    other_players=[]\n",
    "    \n",
    "    for i in soup.find_all('tr',class_=\"table-body\"):\n",
    "        other_players.append(i.text.replace('\\n',' ').split()[2:6])\n",
    "    return other_players\n",
    "other_players(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] [] []\n"
     ]
    }
   ],
   "source": [
    "#c \n",
    "# NO TEST DATA AVAILABLE ON THE SITE\n",
    "url='https://www.icc-cricket.com/rankings/womens/player-rankings/468'\n",
    "\n",
    "def bowlers(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    bowlers=[]\n",
    "    \n",
    "    for i in soup.find_all('div',class_=\"rankings-block__banner--name-large\"):\n",
    "        bowlers.append(i.text)\n",
    "    return bowlers\n",
    "bowlers(url)\n",
    "\n",
    "def teams(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    teams=[]\n",
    "    \n",
    "    for i in soup.find_all('div',class_=\"rankings-block__banner--nationality\"):\n",
    "        teams.append(i.text.replace('\\n',' ').split()[0])\n",
    "    return teams\n",
    "\n",
    "teams(url)\n",
    "\n",
    "def rating(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    rating=[]\n",
    "    \n",
    "    for i in soup.find_all('td',class_=\"u-text-left\"):\n",
    "        rating.append(i.text.split()[0])\n",
    "    return rating\n",
    "rating(url)\n",
    "\n",
    "print(batsmen(url),teams(url),rating(url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) a python program to scrape details of all the posts from https://coreyms.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=('https://coreyms.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heading(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    heading=[]\n",
    "    \n",
    "    for i in soup.find_all('a', class_=\"entry-title-link\"):\n",
    "        heading.append(i.text)\n",
    "    return heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python Tutorial: Zip Files – Creating and Extracting Zip Archives',\n",
       " 'Python Data Science Tutorial: Analyzing the 2019 Stack Overflow Developer Survey',\n",
       " 'Python Multiprocessing Tutorial: Run Code in Parallel Using the Multiprocessing Module',\n",
       " 'Python Threading Tutorial: Run Code Concurrently Using the Threading Module',\n",
       " 'Update (2019-09-03)',\n",
       " 'Python Quick Tip: The Difference Between “==” and “is” (Equality vs Identity)',\n",
       " 'Python Tutorial: Calling External Commands Using the Subprocess Module',\n",
       " 'Visual Studio Code (Windows) – Setting up a Python Development Environment and Complete Overview',\n",
       " 'Visual Studio Code (Mac) – Setting up a Python Development Environment and Complete Overview',\n",
       " 'Clarifying the Issues with Mutable Default Arguments']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heading(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    date=[]\n",
    "    \n",
    "    for i in soup.find_all('time', class_=\"entry-time\"):\n",
    "        date.append(i.text)\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['November 19, 2019',\n",
       " 'October 17, 2019',\n",
       " 'September 21, 2019',\n",
       " 'September 12, 2019',\n",
       " 'September 3, 2019',\n",
       " 'August 6, 2019',\n",
       " 'July 24, 2019',\n",
       " 'May 1, 2019',\n",
       " 'May 1, 2019',\n",
       " 'April 24, 2019']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    content=[]\n",
    "    \n",
    "    for i in soup.find_all('div',class_=\"entry-content\"):\n",
    "        content.append(i.text.replace('\\n',' '))\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' In this video, we will be learning how to create and extract zip archives. We will start by using the zipfile module, and then we will see how to do this using the shutil module. We will learn how to do this with single files and directories, as well as learning how to use gzip as well. Let’s get started…  ',\n",
       " ' In this Python Programming video, we will be learning how to download and analyze real-world data from the 2019 Stack Overflow Developer Survey. This is terrific practice for anyone getting into the data science field. We will learn different ways to analyze this data and also some best practices. Let’s get started…    ',\n",
       " ' In this Python Programming video, we will be learning how to run code in parallel using the multiprocessing module. We will also look at how to process multiple high-resolution images at the same time using a ProcessPoolExecutor from the concurrent.futures module. Let’s get started…    ',\n",
       " ' In this Python Programming video, we will be learning how to run threads concurrently using the threading module. We will also look at how to download multiple high-resolution images online using a ThreadPoolExecutor from the concurrent.futures module. Let’s get started…    ',\n",
       " ' Hey everyone. I wanted to give you an update on my videos. I will be releasing videos on threading and multiprocessing within the next week. Thanks so much for your patience. I currently have a temporary recording studio setup at my Airbnb that will allow me to record and edit the threading/multiprocessing videos. I am going to be moving into my new house in 10 days and once I have my recording studio setup then you can expect much faster video releases. I really appreciate how patient everyone has been while I go through this move, especially those of you who are contributing monthly through YouTube  ',\n",
       " ' In this Python Programming Tutorial, we will be learning the difference between using “==” and the “is” keyword when doing comparisons. The difference between these is that “==” checks to see if values are equal, and the “is” keyword checks their identity, which means it’s going to check if the values are identical in terms of being the same object in memory. We’ll learn more in the video. Let’s get started…    ',\n",
       " ' In this Python Programming Tutorial, we will be learning how to run external commands using the subprocess module from the standard library. We will learn how to run commands, capture the output, handle errors, and also how to pipe output into other commands. Let’s get started…    ',\n",
       " ' In this Python Programming Tutorial, we will be learning how to set up a Python development environment in VSCode on Windows. VSCode is a very nice free editor for writing Python applications and many developers are now switching over to this editor. In this video, we will learn how to install VSCode, get the Python extension installed, how to change Python interpreters, create virtual environments, format/lint our code, how to use Git within VSCode, how to debug our programs, how unit testing works, and more. We have a lot to cover, so let’s go ahead and get started… VSCode on MacOS – https://youtu.be/06I63_p-2A4 Timestamps for topics in this tutorial: Installation – 1:13 Python Extension – 5:48 Switching Interpreters – 10:04 Changing Color Themes – 12:35 VSCode Settings – 16:16 Set Default Python – 21:33 Using Virtual Environments – 25:10 IntelliSense – 29:45 Code Formatting – 32:13 Code Linting – 37:06 Code Runner Extension – 39:42 Git Integration – 47:44 Use Different Terminal – 51:07 Debugging – 58:45 Unit Testing – 1:03:25 Zen Mode – 1:09:55    ',\n",
       " ' In this Python Programming Tutorial, we will be learning how to set up a Python development environment in VSCode on MacOS. VSCode is a very nice free editor for writing Python applications and many developers are now switching over to this editor. In this video, we will learn how to install VSCode, get the Python extension installed, how to change Python interpreters, create virtual environments, format/lint our code, how to use Git within VSCode, how to debug our programs, how unit testing works, and more. We have a lot to cover, so let’s go ahead and get started… VSCode on Windows – https://youtu.be/-nh9rCzPJ20 Timestamps for topics in this tutorial: Installation – 1:11 Python Extension – 6:21 Switching Interpreters – 10:16 Changing Color Themes – 13:08 VSCode Settings – 17:12 Set Default Python – 22:24 Using Virtual Environments – 25:52 IntelliSense – 30:28 Code Formatting – 33:08 Code Linting – 38:01 Code Runner Extension – 40:45 Git Integration – 49:05 Debugging – 58:15 Unit Testing – 1:02:38 Zen Mode – 1:10:42     ',\n",
       " ' In this Python Programming Tutorial, we will be clarifying the issues with mutable default arguments. We discussed this in my last video titled “5 Common Python Mistakes and How to Fix Them”, but I received many comments from people who were still confused. So we will be doing a deeper dive to explain exactly what is going on here. Let’s get started…    ']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    video=[]\n",
    "    \n",
    "    for i in soup.find_all('iframe',class_=\"youtube-player\"):\n",
    "        video.append(i['src'])\n",
    "    return video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.youtube.com/embed/z0gguhEmWiY?version=3&rel=1&showsearch=0&showinfo=1&iv_load_policy=1&fs=1&hl=en-US&autohide=2&wmode=transparent',\n",
       " 'https://www.youtube.com/embed/_P7X8tMplsw?version=3&rel=1&showsearch=0&showinfo=1&iv_load_policy=1&fs=1&hl=en-US&autohide=2&wmode=transparent',\n",
       " 'https://www.youtube.com/embed/fKl2JW_qrso?version=3&rel=1&showsearch=0&showinfo=1&iv_load_policy=1&fs=1&hl=en-US&autohide=2&wmode=transparent',\n",
       " 'https://www.youtube.com/embed/IEEhzQoKtQU?version=3&rel=1&showsearch=0&showinfo=1&iv_load_policy=1&fs=1&hl=en-US&autohide=2&wmode=transparent',\n",
       " 'https://www.youtube.com/embed/mO_dS3rXDIs?version=3&rel=1&showsearch=0&showinfo=1&iv_load_policy=1&fs=1&hl=en-US&autohide=2&wmode=transparent',\n",
       " 'https://www.youtube.com/embed/2Fp1N6dof0Y?version=3&rel=1&showsearch=0&showinfo=1&iv_load_policy=1&fs=1&hl=en-US&autohide=2&wmode=transparent',\n",
       " 'https://www.youtube.com/embed/-nh9rCzPJ20?version=3&rel=1&showsearch=0&showinfo=1&iv_load_policy=1&fs=1&hl=en-US&autohide=2&wmode=transparent',\n",
       " 'https://www.youtube.com/embed/06I63_p-2A4?version=3&rel=1&showsearch=0&showinfo=1&iv_load_policy=1&fs=1&hl=en-US&autohide=2&wmode=transparent',\n",
       " 'https://www.youtube.com/embed/_JGmemuINww?version=3&rel=1&showsearch=0&showinfo=1&iv_load_policy=1&fs=1&hl=en-US&autohide=2&wmode=transparent']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Heading</th>\n",
       "      <th>Date</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Python Tutorial: Zip Files – Creating and Extr...</td>\n",
       "      <td>November 19, 2019</td>\n",
       "      <td>In this video, we will be learning how to cre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Python Data Science Tutorial: Analyzing the 20...</td>\n",
       "      <td>October 17, 2019</td>\n",
       "      <td>In this Python Programming video, we will be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Python Multiprocessing Tutorial: Run Code in P...</td>\n",
       "      <td>September 21, 2019</td>\n",
       "      <td>In this Python Programming video, we will be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Python Threading Tutorial: Run Code Concurrent...</td>\n",
       "      <td>September 12, 2019</td>\n",
       "      <td>In this Python Programming video, we will be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Update (2019-09-03)</td>\n",
       "      <td>September 3, 2019</td>\n",
       "      <td>Hey everyone. I wanted to give you an update ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Python Quick Tip: The Difference Between “==” ...</td>\n",
       "      <td>August 6, 2019</td>\n",
       "      <td>In this Python Programming Tutorial, we will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Python Tutorial: Calling External Commands Usi...</td>\n",
       "      <td>July 24, 2019</td>\n",
       "      <td>In this Python Programming Tutorial, we will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Visual Studio Code (Windows) – Setting up a Py...</td>\n",
       "      <td>May 1, 2019</td>\n",
       "      <td>In this Python Programming Tutorial, we will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Visual Studio Code (Mac) – Setting up a Python...</td>\n",
       "      <td>May 1, 2019</td>\n",
       "      <td>In this Python Programming Tutorial, we will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Clarifying the Issues with Mutable Default Arg...</td>\n",
       "      <td>April 24, 2019</td>\n",
       "      <td>In this Python Programming Tutorial, we will ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Heading                Date  \\\n",
       "0  Python Tutorial: Zip Files – Creating and Extr...   November 19, 2019   \n",
       "1  Python Data Science Tutorial: Analyzing the 20...    October 17, 2019   \n",
       "2  Python Multiprocessing Tutorial: Run Code in P...  September 21, 2019   \n",
       "3  Python Threading Tutorial: Run Code Concurrent...  September 12, 2019   \n",
       "4                                Update (2019-09-03)   September 3, 2019   \n",
       "5  Python Quick Tip: The Difference Between “==” ...      August 6, 2019   \n",
       "6  Python Tutorial: Calling External Commands Usi...       July 24, 2019   \n",
       "7  Visual Studio Code (Windows) – Setting up a Py...         May 1, 2019   \n",
       "8  Visual Studio Code (Mac) – Setting up a Python...         May 1, 2019   \n",
       "9  Clarifying the Issues with Mutable Default Arg...      April 24, 2019   \n",
       "\n",
       "                                             Content  \n",
       "0   In this video, we will be learning how to cre...  \n",
       "1   In this Python Programming video, we will be ...  \n",
       "2   In this Python Programming video, we will be ...  \n",
       "3   In this Python Programming video, we will be ...  \n",
       "4   Hey everyone. I wanted to give you an update ...  \n",
       "5   In this Python Programming Tutorial, we will ...  \n",
       "6   In this Python Programming Tutorial, we will ...  \n",
       "7   In this Python Programming Tutorial, we will ...  \n",
       "8   In this Python Programming Tutorial, we will ...  \n",
       "9   In this Python Programming Tutorial, we will ...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Heading':heading(url),'Date':date(url),'Content':content(url)})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) a python program to scrape house details from mentioned URL. It should include house title, location,area, EMI and price from https://www.nobroker.in/ .Enter three localities which are Indira Nagar, Jayanagar, Rajaji Nagar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=('https://www.nobroker.in/property/sale/bangalore/multiple?searchParam=W3sibGF0IjoxMi45NzgzNjkyLCJsb24iOjc3LjY0MDgzNTYsInBsYWNlSWQiOiJDaElKa1FOM0dLUVdyanNSTmhCUUpyaEdEN1UiLCJwbGFjZU5hbWUiOiJJbmRpcmFuYWdhciIsInNob3dNYXAiOmZhbHNlfSx7ImxhdCI6MTIuOTk4MTczMiwibG9uIjo3Ny41NTMwNDQ1OTk5OTk5OSwicGxhY2VJZCI6IkNoSUp4Zlc0RFBNOXJqc1JLc05URy01cF9RUSIsInBsYWNlTmFtZSI6IlJhamFqaW5hZ2FyIiwic2hvd01hcCI6ZmFsc2V9LHsibGF0IjoxMi45MzA3NzM1LCJsb24iOjc3LjU4MzgzMDIsInBsYWNlSWQiOiJDaElKMmRkbFo1Z1ZyanNSaDFCT0FhZi1vcnMiLCJwbGFjZU5hbWUiOiJKYXlhbmFnYXIiLCJzaG93TWFwIjpmYWxzZX1d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    title=[]\n",
    "    \n",
    "    for i in soup.find_all('span' ,class_=\"overflow-hidden overflow-ellipsis whitespace-nowrap max-w-80pe po:max-w-full\"):\n",
    "        title.append(i.text)\n",
    "\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3 BHK Apartment  For Sale  In Ashiana Gardens Apartments In Indiranagar',\n",
       " '3 BHK Flat  For Sale  In Sobha Indraprastha, Rajaji Nagar In Rajaji Nagar',\n",
       " '2 BHK In Independent House  For Sale  In 9th Block Jayanagara',\n",
       " '3 BHK Flat  For Sale  In  Sgrr Pallavi Pristine In Jayanagar',\n",
       " '3 BHK Flat  For Sale  In Total Environment Life Is Beautiful, Jayanagar In Jayanagar']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    location=[]\n",
    "    \n",
    "    for i in soup.find_all('div', class_=\"mt-0.5p overflow-hidden overflow-ellipsis whitespace-nowrap max-w-70 text-gray-light leading-4 po:mb-0 po:max-w-95\"):\n",
    "        location.append(i.text)\n",
    "\n",
    "    return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ashiana Gardens Apartments\\xa0 4, Sri Rama Temple Rd, Channakesahava Nagar, HAL 2nd Stage, Doopanahalli, Indiranagar, Bengaluru, Karnataka 560008, India',\n",
       " 'Minerva Mills Compound,Opposite Sujata Theatre, Rajaji Nagar, Bengaluru, Karnataka 560023, India',\n",
       " 'Independent House, 41 A Cross, 18th Main, Jayanagar 9th Block, BLR-560041',\n",
       " '11th Main Road,36 Cross, 4th T Block, Near ,Post office, Jayanagar HPO',\n",
       " '30th Cross Rd, 4th T Block , Jayanagar, Bengaluru, Karnataka 560041, India']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emi(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    emi=[]\n",
    "    \n",
    "    for i in soup.find_all('div',class_=\"font-semi-bold heading-6\"):\n",
    "        emi.append(i.text)\n",
    "\n",
    "    return emi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,780 sqft',\n",
       " '₹80,240/Month',\n",
       " '₹1.4 Crores',\n",
       " '2,200 sqft',\n",
       " '₹2.06 Lacs/Month',\n",
       " '₹3.6 Crores',\n",
       " '1,500 sqft',\n",
       " '₹65,911/Month',\n",
       " '₹1.15 Crores',\n",
       " '1,340 sqft',\n",
       " '₹74,508/Month',\n",
       " '₹1.3 Crores',\n",
       " '2,628 sqft',\n",
       " '₹1.72 Lacs/Month',\n",
       " '₹3 Crores']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emi(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    price=[]\n",
    "    \n",
    "    for i in soup.find_all('div',class_=\"font-semi-bold heading-6\"):\n",
    "        price.append(i.text)\n",
    "\n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,780 sqft',\n",
       " '₹80,240/Month',\n",
       " '₹1.4 Crores',\n",
       " '2,200 sqft',\n",
       " '₹2.06 Lacs/Month',\n",
       " '₹3.6 Crores',\n",
       " '1,500 sqft',\n",
       " '₹65,911/Month',\n",
       " '₹1.15 Crores',\n",
       " '1,340 sqft',\n",
       " '₹74,508/Month',\n",
       " '₹1.3 Crores',\n",
       " '2,628 sqft',\n",
       " '₹1.72 Lacs/Month',\n",
       " '₹3 Crores']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9)  a python program to scrape mentioned details from https://www.dineout.co.in/delhi-restaurants/buffet-special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=('https://www.dineout.co.in/delhi-restaurants/buffet-special')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def titles(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    titles=[]\n",
    "    \n",
    "    for i in soup.find_all('a', class_=\"restnt-name ellipsis\"):\n",
    "        titles.append(i.text)\n",
    "\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Castle Barbeque',\n",
       " 'Jungle Jamboree',\n",
       " 'Cafe Knosh',\n",
       " 'Castle Barbeque',\n",
       " 'The Barbeque Company',\n",
       " 'India Grill',\n",
       " 'Delhi Barbeque',\n",
       " 'The Monarch - Bar Be Que Village',\n",
       " 'World Cafe',\n",
       " 'Indian Grill Room',\n",
       " 'Mad 4 Bar B Que',\n",
       " 'Barbeque 29',\n",
       " 'Glasshouse']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuisine(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    cuisine=[]\n",
    "    \n",
    "    for i in soup.find_all('span',class_=\"double-line-ellipsis\"):\n",
    "        cuisine.append(i.text.split()[7:])\n",
    "\n",
    "    return cuisine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Indian,', 'Chinese'],\n",
       " ['Indian,', 'Asian,', 'Italian'],\n",
       " ['Continental'],\n",
       " ['North', 'Indian'],\n",
       " ['Indian,', 'Chinese'],\n",
       " ['Indian,', 'Italian'],\n",
       " ['Indian'],\n",
       " ['Indian,', 'Chinese'],\n",
       " ['Indian,', 'Chinese,', 'Continental'],\n",
       " ['Indian,', 'Mughlai'],\n",
       " ['Indian,', 'Mughlai'],\n",
       " ['Indian,', 'Chinese'],\n",
       " ['Italian,', 'Asian,', 'Continental']]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuisine(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    location=[]\n",
    "    \n",
    "    for i in soup.find_all('div',class_=\"restnt-loc ellipsis\"):\n",
    "        location.append(i.text)\n",
    "\n",
    "    return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Connaught Place, Central Delhi',\n",
       " '3CS Mall,Lajpat Nagar - 3, South Delhi',\n",
       " 'The Leela Ambience Convention Hotel,Shahdara, East Delhi',\n",
       " 'Pacific Mall,Tagore Garden, West Delhi',\n",
       " 'Gardens Galleria,Sector 38A, Noida',\n",
       " 'Hilton Garden Inn,Saket, South Delhi',\n",
       " 'Taurus Sarovar Portico,Mahipalpur, South Delhi',\n",
       " 'Indirapuram Habitat Centre,Indirapuram, Ghaziabad',\n",
       " 'Vibe by The Lalit Traveller,Sector 35, Faridabad',\n",
       " 'Suncity Business Tower,Golf Course Road, Gurgaon',\n",
       " 'Sector 29, Faridabad',\n",
       " 'NIT, Faridabad',\n",
       " 'DoubleTree By Hilton Gurugram Baani Square,Sector 50, Gurgaon']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratings(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    ratings=[]\n",
    "    \n",
    "    for i in soup.find_all('div',class_=\"restnt-rating rating-4\"):\n",
    "        ratings.append(i.text)\n",
    "\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3.5',\n",
       " '3.9',\n",
       " '4.3',\n",
       " '3.9',\n",
       " '4',\n",
       " '3.9',\n",
       " '3.7',\n",
       " '3.9',\n",
       " '4.3',\n",
       " '4.3',\n",
       " '3.6',\n",
       " '4.2',\n",
       " '4.1']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://im1.dineout.co.in/images/uploads/restaurant/sharpen/8/k/b/p86792-16062953735fbe1f4d3fb7e.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/5/a/k/p59633-16046474755fa4fa33c0e92.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/4/p/m/p406-15438184745c04ccea491bc.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/3/j/o/p38113-15959192065f1fcb666130c.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/7/q/d/p79307-16051787075fad15532bd7c.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/2/v/t/p2687-1482477169585cce712b90f.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/5/v/f/p52501-16006856545f68865616659.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/3/n/o/p34822-15599107305cfa594a13c24.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/1/p/y/p12366-1466935020576fa6ecdc359.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/5/y/d/p549-15291237715b2493bb2a415.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/4/j/e/p43488-15295778165b2b8158ceeef.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/5/w/r/p58842-15624171585d209806d9143.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/9/m/a/p9875-1645177960620f6c68ecfef.jpg?tr=tr:n-medium']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page=requests.get('https://www.dineout.co.in/delhi-restaurants/buffet-special')\n",
    "soup=BeautifulSoup(page.content)\n",
    "images=[]\n",
    "\n",
    "for i in soup.find_all('img', class_=\"no-img\"):\n",
    "    images.append(i['data-src'])\n",
    "\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant Name</th>\n",
       "      <th>Cuisine</th>\n",
       "      <th>Location</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>[Indian,, Chinese]</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>3.5</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jungle Jamboree</td>\n",
       "      <td>[Indian,, Asian,, Italian]</td>\n",
       "      <td>3CS Mall,Lajpat Nagar - 3, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cafe Knosh</td>\n",
       "      <td>[Continental]</td>\n",
       "      <td>The Leela Ambience Convention Hotel,Shahdara, ...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>[North, Indian]</td>\n",
       "      <td>Pacific Mall,Tagore Garden, West Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Barbeque Company</td>\n",
       "      <td>[Indian,, Chinese]</td>\n",
       "      <td>Gardens Galleria,Sector 38A, Noida</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>India Grill</td>\n",
       "      <td>[Indian,, Italian]</td>\n",
       "      <td>Hilton Garden Inn,Saket, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Delhi Barbeque</td>\n",
       "      <td>[Indian]</td>\n",
       "      <td>Taurus Sarovar Portico,Mahipalpur, South Delhi</td>\n",
       "      <td>3.7</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Monarch - Bar Be Que Village</td>\n",
       "      <td>[Indian,, Chinese]</td>\n",
       "      <td>Indirapuram Habitat Centre,Indirapuram, Ghaziabad</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>World Cafe</td>\n",
       "      <td>[Indian,, Chinese,, Continental]</td>\n",
       "      <td>Vibe by The Lalit Traveller,Sector 35, Faridabad</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Indian Grill Room</td>\n",
       "      <td>[Indian,, Mughlai]</td>\n",
       "      <td>Suncity Business Tower,Golf Course Road, Gurgaon</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Mad 4 Bar B Que</td>\n",
       "      <td>[Indian,, Mughlai]</td>\n",
       "      <td>Sector 29, Faridabad</td>\n",
       "      <td>3.6</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Barbeque 29</td>\n",
       "      <td>[Indian,, Chinese]</td>\n",
       "      <td>NIT, Faridabad</td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Glasshouse</td>\n",
       "      <td>[Italian,, Asian,, Continental]</td>\n",
       "      <td>DoubleTree By Hilton Gurugram Baani Square,Sec...</td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Restaurant Name                           Cuisine  \\\n",
       "0                    Castle Barbeque                [Indian,, Chinese]   \n",
       "1                    Jungle Jamboree        [Indian,, Asian,, Italian]   \n",
       "2                         Cafe Knosh                     [Continental]   \n",
       "3                    Castle Barbeque                   [North, Indian]   \n",
       "4               The Barbeque Company                [Indian,, Chinese]   \n",
       "5                        India Grill                [Indian,, Italian]   \n",
       "6                     Delhi Barbeque                          [Indian]   \n",
       "7   The Monarch - Bar Be Que Village                [Indian,, Chinese]   \n",
       "8                         World Cafe  [Indian,, Chinese,, Continental]   \n",
       "9                  Indian Grill Room                [Indian,, Mughlai]   \n",
       "10                   Mad 4 Bar B Que                [Indian,, Mughlai]   \n",
       "11                       Barbeque 29                [Indian,, Chinese]   \n",
       "12                        Glasshouse   [Italian,, Asian,, Continental]   \n",
       "\n",
       "                                             Location Rating  \\\n",
       "0                      Connaught Place, Central Delhi    3.5   \n",
       "1              3CS Mall,Lajpat Nagar - 3, South Delhi    3.9   \n",
       "2   The Leela Ambience Convention Hotel,Shahdara, ...    4.3   \n",
       "3              Pacific Mall,Tagore Garden, West Delhi    3.9   \n",
       "4                  Gardens Galleria,Sector 38A, Noida      4   \n",
       "5                Hilton Garden Inn,Saket, South Delhi    3.9   \n",
       "6      Taurus Sarovar Portico,Mahipalpur, South Delhi    3.7   \n",
       "7   Indirapuram Habitat Centre,Indirapuram, Ghaziabad    3.9   \n",
       "8    Vibe by The Lalit Traveller,Sector 35, Faridabad    4.3   \n",
       "9    Suncity Business Tower,Golf Course Road, Gurgaon    4.3   \n",
       "10                               Sector 29, Faridabad    3.6   \n",
       "11                                     NIT, Faridabad    4.2   \n",
       "12  DoubleTree By Hilton Gurugram Baani Square,Sec...    4.1   \n",
       "\n",
       "                                               Images  \n",
       "0   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "1   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "2   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "3   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "4   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "5   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "6   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "7   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "8   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "9   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "10  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "11  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "12  https://im1.dineout.co.in/images/uploads/resta...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Restaurant Name':titles(url),'Cuisine':cuisine(url),'Location':location(url),'Rating':ratings(url),'Images':images})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10)  a python program to scrape first 10 product details which include product name , price , Image URL fromhttps://www.bewakoof.com/women-tshirts?ga_q=tshirts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=('https://www.bewakoof.com/women-tshirts?ga_q=tshirts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    name=[]\n",
    "    \n",
    "    for i in soup.find_all('div', class_=\"productCardDetail \"):\n",
    "        name.append(i.text)\n",
    "\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    price=[]\n",
    "    \n",
    "    for i in soup.find_all('span',class_=\"discountedPriceText \"):\n",
    "        price.append(i.text)\n",
    "\n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "    images=[]\n",
    "    \n",
    "    for i in soup.find_all('img', class_=\"productImgTag\"):\n",
    "        images.append(i['src'])\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://images.bewakoof.com/t320/inner-peace-half-sleeve-printed-t-shirts-black-women-s-half-sleeve-printed-t-shirt-295708-1604135493.jpg',\n",
       " 'https://images.bewakoof.com/t320/sleepy-cat-half-sleeve-printed-t-shirts-black-women-s-half-sleeve-printed-t-shirt-295717-1603970467.jpg',\n",
       " 'https://images.bewakoof.com/t320/peace-jerry-half-sleeve-printed-t-shirt-tjl-white-women-s-half-sleeve-printed-t-shirt-295801-1603465610.jpg',\n",
       " 'https://images.bewakoof.com/t320/donut-worry-half-sleeve-t-shirt-pineapple-yellow-women-s-half-sleeve-printed-t-shirt-291941-1602573790.jpg',\n",
       " 'https://images.bewakoof.com/t320/digital-teal-round-neck-t-shirt-women-s-half-sleeve-plain-t-shirt-292154-1604125781.jpg',\n",
       " 'https://images.bewakoof.com/t320/all-we-have-half-sleeve-t-shirt-meteor-grey-women-s-half-sleeve-printed-t-shirt-287646-1601034494.jpg',\n",
       " 'https://images.bewakoof.com/t320/simply-pawfect-half-sleeve-t-shirt-dl-tropical-blue-women-s-half-sleeve-printed-t-shirt-295812-1604292711.jpg',\n",
       " 'https://images.bewakoof.com/t320/kolkata-biryani-boyfriend-t-shirt-women-s-printed-boyfriend-t-shirts-329659-1614078513.jpg',\n",
       " 'https://images.bewakoof.com/t320/hobe-naki-ek-bar-half-sleeve-t-shirt-women-s-half-sleeve-printed-t-shirt-329660-1614078109.jpg',\n",
       " 'https://images.bewakoof.com/t320/ray-half-sleeve-t-shirt-women-s-half-sleeve-printed-t-shirt-329664-1614079574.jpg']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
